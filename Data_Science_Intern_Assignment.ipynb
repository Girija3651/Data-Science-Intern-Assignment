{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1489a86d-39e1-41b7-894c-69fc4fd0c40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Business Insights:\n",
      "\n",
      "1. Customer Geographic Distribution:\n",
      "Region\n",
      "South America    59\n",
      "Europe           50\n",
      "North America    46\n",
      "Asia             45\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2. Top Product Categories by Sales:\n",
      "Category\n",
      "Books          192147.47\n",
      "Electronics    180783.50\n",
      "Clothing       166170.66\n",
      "Home Decor     150893.93\n",
      "Name: TotalValue, dtype: float64\n",
      "\n",
      "3. Average Transaction Value by Region:\n",
      "Region\n",
      "Asia             697.591606\n",
      "Europe           710.489872\n",
      "North America    624.235246\n",
      "South America    721.554474\n",
      "Name: TotalValue, dtype: float64\n",
      "\n",
      "4. Product Price Statistics:\n",
      "count    100.000000\n",
      "mean     267.551700\n",
      "std      143.219383\n",
      "min       16.080000\n",
      "25%      147.767500\n",
      "50%      292.875000\n",
      "75%      397.090000\n",
      "max      497.760000\n",
      "Name: Price, dtype: float64\n",
      "\n",
      "5. Average Purchase Frequency per Customer:\n",
      "Average number of transactions per customer: 5.03\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Read the CSV files\n",
    "customers_df = pd.read_csv('Customers.csv')\n",
    "transactions_df = pd.read_csv('Transactions.csv')\n",
    "products_df = pd.read_csv('Products.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "customers_df['SignupDate'] = pd.to_datetime(customers_df['SignupDate'])\n",
    "transactions_df['TransactionDate'] = pd.to_datetime(transactions_df['TransactionDate'])\n",
    "\n",
    "# Customer Analysis\n",
    "def analyze_customers():\n",
    "    # Region distribution\n",
    "    region_dist = customers_df['Region'].value_counts()\n",
    "    \n",
    "    # Customer signup trends\n",
    "    customers_df['SignupMonth'] = customers_df['SignupDate'].dt.to_period('M')\n",
    "    monthly_signups = customers_df.groupby('SignupMonth').size()\n",
    "    \n",
    "    return region_dist, monthly_signups\n",
    "\n",
    "# Transaction Analysis\n",
    "def analyze_transactions():\n",
    "    # Merge transactions with customer and product information\n",
    "    merged_df = transactions_df.merge(customers_df, on='CustomerID')\n",
    "    merged_df = merged_df.merge(products_df, on='ProductID')\n",
    "    \n",
    "    # Calculate customer lifetime value (CLV)\n",
    "    customer_ltv = merged_df.groupby('CustomerID')['TotalValue'].sum().sort_values(ascending=False)\n",
    "    \n",
    "    # Analyze product categories\n",
    "    category_sales = merged_df.groupby('Category')['TotalValue'].sum().sort_values(ascending=False)\n",
    "    \n",
    "    # Calculate average transaction value by region\n",
    "    avg_transaction_by_region = merged_df.groupby('Region')['TotalValue'].mean()\n",
    "    \n",
    "    # Analyze purchase frequency\n",
    "    purchase_frequency = merged_df.groupby('CustomerID').size().mean()\n",
    "    \n",
    "    return customer_ltv, category_sales, avg_transaction_by_region, purchase_frequency\n",
    "\n",
    "# Product Analysis\n",
    "def analyze_products():\n",
    "    # Product category distribution\n",
    "    category_dist = products_df['Category'].value_counts()\n",
    "    \n",
    "    # Price distribution statistics\n",
    "    price_stats = products_df['Price'].describe()\n",
    "    \n",
    "    return category_dist, price_stats\n",
    "\n",
    "# Generate visualizations\n",
    "def create_visualizations():\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Customer region distribution\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.countplot(data=customers_df, x='Region')\n",
    "    plt.title('Customer Distribution by Region')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Transaction value distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.histplot(data=transactions_df, x='TotalValue', bins=30)\n",
    "    plt.title('Transaction Value Distribution')\n",
    "    \n",
    "    # Product category distribution\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.countplot(data=products_df, x='Category')\n",
    "    plt.title('Product Category Distribution')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eda_visualizations.png')\n",
    "\n",
    "# Run the analysis\n",
    "region_dist, monthly_signups = analyze_customers()\n",
    "customer_ltv, category_sales, avg_transaction_by_region, purchase_frequency = analyze_transactions()\n",
    "category_dist, price_stats = analyze_products()\n",
    "\n",
    "# Print insights\n",
    "print(\"\\nBusiness Insights:\")\n",
    "print(\"\\n1. Customer Geographic Distribution:\")\n",
    "print(region_dist)\n",
    "\n",
    "print(\"\\n2. Top Product Categories by Sales:\")\n",
    "print(category_sales)\n",
    "\n",
    "print(\"\\n3. Average Transaction Value by Region:\")\n",
    "print(avg_transaction_by_region)\n",
    "\n",
    "print(\"\\n4. Product Price Statistics:\")\n",
    "print(price_stats)\n",
    "\n",
    "print(\"\\n5. Average Purchase Frequency per Customer:\")\n",
    "print(f\"Average number of transactions per customer: {purchase_frequency:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "993dbee0-3055-4125-91ea-b299789ed3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating customer profiles...\n",
      "Generating lookalike recommendations...\n",
      "Saving results...\n",
      "\n",
      "Sample lookalike recommendations:\n",
      "\n",
      "Target Customer: C0001\n",
      "Similar Customer: C0005, Similarity Score: 0.7979\n",
      "Similar Customer: C0069, Similarity Score: 0.7179\n",
      "Similar Customer: C0130, Similarity Score: 0.6831\n",
      "\n",
      "Target Customer: C0002\n",
      "Similar Customer: C0060, Similarity Score: 0.7916\n",
      "Similar Customer: C0062, Similarity Score: 0.7798\n",
      "Similar Customer: C0025, Similarity Score: 0.7519\n",
      "\n",
      "Target Customer: C0003\n",
      "Similar Customer: C0144, Similarity Score: 0.9027\n",
      "Similar Customer: C0091, Similarity Score: 0.7028\n",
      "Similar Customer: C0151, Similarity Score: 0.6788\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "# Read the data\n",
    "customers_df = pd.read_csv('Customers.csv')\n",
    "transactions_df = pd.read_csv('Transactions.csv')\n",
    "products_df = pd.read_csv('Products.csv')\n",
    "\n",
    "def create_customer_profile():\n",
    "    \"\"\"Create comprehensive customer profiles using both customer and transaction data\"\"\"\n",
    "    \n",
    "    # 1. Basic customer features from customer data\n",
    "    customer_profile = customers_df.copy()\n",
    "    customer_profile['SignupDate'] = pd.to_datetime(customer_profile['SignupDate'])\n",
    "    customer_profile['account_age_days'] = (pd.Timestamp.now() - customer_profile['SignupDate']).dt.days\n",
    "    \n",
    "    # 2. Transaction-based features\n",
    "    transaction_features = transactions_df.groupby('CustomerID').agg({\n",
    "        'TransactionID': 'count',  # number of transactions\n",
    "        'TotalValue': ['sum', 'mean', 'std'],  # spending patterns\n",
    "        'Quantity': ['sum', 'mean', 'std'],  # purchase quantity patterns\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    transaction_features.columns = [\n",
    "        'CustomerID', 'transaction_count', 'total_spend', 'avg_transaction_value',\n",
    "        'std_transaction_value', 'total_items', 'avg_items_per_transaction', 'std_items'\n",
    "    ]\n",
    "    \n",
    "    # 3. Product category preferences\n",
    "    # Merge transactions with products to get categories\n",
    "    trans_with_categories = transactions_df.merge(\n",
    "        products_df[['ProductID', 'Category']], \n",
    "        on='ProductID'\n",
    "    )\n",
    "    \n",
    "    # Calculate category preferences using pivot_table\n",
    "    category_pivot = pd.pivot_table(\n",
    "        trans_with_categories,\n",
    "        index='CustomerID',\n",
    "        columns='Category',\n",
    "        values='Quantity',\n",
    "        aggfunc='sum',\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Normalize category preferences\n",
    "    category_sums = category_pivot.sum(axis=1)\n",
    "    category_pivot = category_pivot.div(category_sums, axis=0).fillna(0)\n",
    "    \n",
    "    # 4. Recent behavior features\n",
    "    trans_with_categories['TransactionDate'] = pd.to_datetime(trans_with_categories['TransactionDate'])\n",
    "    recent_transactions = trans_with_categories.sort_values('TransactionDate').groupby('CustomerID').tail(5)\n",
    "    \n",
    "    recent_features = recent_transactions.groupby('CustomerID').agg({\n",
    "        'TotalValue': 'mean',\n",
    "        'Quantity': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    recent_features.columns = ['CustomerID', 'recent_avg_value', 'recent_avg_quantity']\n",
    "    \n",
    "    # Add average transaction value per item for recent transactions\n",
    "    recent_features['recent_avg_price_per_item'] = recent_features['recent_avg_value'] / recent_features['recent_avg_quantity']\n",
    "    \n",
    "    # 5. Merge all features\n",
    "    customer_profile = customer_profile.merge(transaction_features, on='CustomerID', how='left')\n",
    "    customer_profile = customer_profile.merge(category_pivot.reset_index(), on='CustomerID', how='left')\n",
    "    customer_profile = customer_profile.merge(recent_features, on='CustomerID', how='left')\n",
    "    \n",
    "    # Fill NaN values for customers with no transactions\n",
    "    customer_profile = customer_profile.fillna(0)\n",
    "    \n",
    "    return customer_profile\n",
    "\n",
    "def find_lookalikes(customer_profile, target_customers, n_recommendations=3):\n",
    "    \"\"\"Find lookalike customers using cosine similarity\"\"\"\n",
    "    \n",
    "    # Select features for similarity calculation\n",
    "    feature_cols = customer_profile.select_dtypes(include=['float64', 'int64']).columns\n",
    "    feature_cols = feature_cols.drop(['CustomerID']) if 'CustomerID' in feature_cols else feature_cols\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    normalized_features = scaler.fit_transform(customer_profile[feature_cols])\n",
    "    \n",
    "    # Calculate similarity matrix\n",
    "    similarity_matrix = cosine_similarity(normalized_features)\n",
    "    \n",
    "    # Generate recommendations for target customers\n",
    "    lookalike_results = {}\n",
    "    customer_ids = customer_profile['CustomerID'].values\n",
    "    \n",
    "    for target_id in target_customers:\n",
    "        target_idx = customer_profile[customer_profile['CustomerID'] == target_id].index[0]\n",
    "        similarities = similarity_matrix[target_idx]\n",
    "        \n",
    "        # Get top N similar customers (excluding self)\n",
    "        similar_indices = np.argsort(similarities)[::-1][1:n_recommendations+1]\n",
    "        \n",
    "        similar_customers = [\n",
    "            {\n",
    "                'customer_id': str(customer_ids[idx]),\n",
    "                'similarity_score': float(similarities[idx])\n",
    "            }\n",
    "            for idx in similar_indices\n",
    "        ]\n",
    "        \n",
    "        lookalike_results[str(target_id)] = similar_customers\n",
    "    \n",
    "    return lookalike_results\n",
    "\n",
    "# Create customer profiles\n",
    "print(\"Creating customer profiles...\")\n",
    "customer_profile = create_customer_profile()\n",
    "\n",
    "# Get first 20 customers\n",
    "target_customers = customers_df['CustomerID'].iloc[:20].tolist()\n",
    "\n",
    "# Generate lookalike recommendations\n",
    "print(\"Generating lookalike recommendations...\")\n",
    "lookalike_results = find_lookalikes(customer_profile, target_customers)\n",
    "\n",
    "# Save results to CSV\n",
    "print(\"Saving results...\")\n",
    "with open('FirstName_LastName_Lookalike.csv', 'w') as f:\n",
    "    json.dump(lookalike_results, f, indent=2)\n",
    "\n",
    "# Print sample results\n",
    "print(\"\\nSample lookalike recommendations:\")\n",
    "for target_id in list(lookalike_results.keys())[:3]:\n",
    "    print(f\"\\nTarget Customer: {target_id}\")\n",
    "    for rec in lookalike_results[target_id]:\n",
    "        print(f\"Similar Customer: {rec['customer_id']}, Similarity Score: {rec['similarity_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e7e9c9a-25c7-4cea-907b-8aeffbeb40fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Preparing features for clustering...\n",
      "\n",
      "2. Performing clustering analysis...\n",
      "\n",
      "3. Clustering Results:\n",
      "✓ Optimal number of clusters: 7\n",
      "✓ Davies-Bouldin Index: 1.4300\n",
      "✓ Silhouette Score: 0.1727\n",
      "✓ Calinski-Harabasz Score: 31.8906\n",
      "\n",
      "4. Cluster sizes:\n",
      "Cluster\n",
      "0    55\n",
      "1    32\n",
      "2     9\n",
      "3    43\n",
      "4    18\n",
      "5    32\n",
      "6    11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "5. Creating visualizations and saving results...\n",
      "\n",
      "Generating visualizations...\n",
      "✓ Created PCA visualization (clustering_pca.png)\n",
      "✓ Created cluster sizes visualization (clustering_sizes.png)\n",
      "✓ Created cluster characteristics heatmap (clustering_characteristics.png)\n",
      "✓ Saved cluster insights to clustering_insights.csv\n",
      "✓ Saved clustering results to FirstName_LastName_Clustering_Results.csv\n",
      "✓ Saved clustering metrics to FirstName_LastName_Clustering_Metrics.csv\n",
      "\n",
      "Clustering analysis completed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score, silhouette_score, calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Set environment variable to avoid KMeans memory leak warning\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "# Filter warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Rest of the code remains the same until visualize_clusters function...\n",
    "\n",
    "def visualize_clusters(features, X, kmeans, save_prefix='clustering'):\n",
    "    \"\"\"Create visualizations for the clustering results\"\"\"\n",
    "    \n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    \n",
    "    # 1. PCA visualization\n",
    "    try:\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(X)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=features['Cluster'], cmap='viridis')\n",
    "        plt.title('Cluster Visualization using PCA')\n",
    "        plt.xlabel('First Principal Component')\n",
    "        plt.ylabel('Second Principal Component')\n",
    "        plt.colorbar(scatter)\n",
    "        plt.savefig(f'{save_prefix}_pca.png')\n",
    "        plt.close()\n",
    "        print(\"✓ Created PCA visualization (clustering_pca.png)\")\n",
    "    except Exception as e:\n",
    "        print(f\"× Error creating PCA visualization: {str(e)}\")\n",
    "    \n",
    "    # 2. Cluster sizes visualization\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        features['Cluster'].value_counts().sort_index().plot(kind='bar')\n",
    "        plt.title('Cluster Sizes')\n",
    "        plt.xlabel('Cluster')\n",
    "        plt.ylabel('Number of Customers')\n",
    "        plt.savefig(f'{save_prefix}_sizes.png')\n",
    "        plt.close()\n",
    "        print(\"✓ Created cluster sizes visualization (clustering_sizes.png)\")\n",
    "    except Exception as e:\n",
    "        print(f\"× Error creating cluster sizes visualization: {str(e)}\")\n",
    "    \n",
    "    # 3. Feature importance heatmap\n",
    "    try:\n",
    "        feature_cols = [\n",
    "            'recency', 'frequency', 'monetary', \n",
    "            'avg_transaction_value', 'avg_items_per_transaction'\n",
    "        ]\n",
    "        \n",
    "        cluster_means = features.groupby('Cluster')[feature_cols].mean()\n",
    "        \n",
    "        # Scale the means for better visualization\n",
    "        scaler = StandardScaler()\n",
    "        cluster_means_scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(cluster_means),\n",
    "            index=cluster_means.index,\n",
    "            columns=cluster_means.columns\n",
    "        )\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))\n",
    "        sns.heatmap(cluster_means_scaled, cmap='YlOrRd', annot=True, fmt='.2f')\n",
    "        plt.title('Cluster Characteristics (Standardized Values)')\n",
    "        plt.savefig(f'{save_prefix}_characteristics.png')\n",
    "        plt.close()\n",
    "        print(\"✓ Created cluster characteristics heatmap (clustering_characteristics.png)\")\n",
    "    except Exception as e:\n",
    "        print(f\"× Error creating characteristics heatmap: {str(e)}\")\n",
    "\n",
    "    # 4. Save cluster insights\n",
    "    try:\n",
    "        cluster_insights = pd.DataFrame()\n",
    "        for col in ['monetary', 'frequency', 'recency']:\n",
    "            cluster_insights[f'avg_{col}'] = features.groupby('Cluster')[col].mean()\n",
    "        cluster_insights.to_csv(f'{save_prefix}_insights.csv')\n",
    "        print(\"✓ Saved cluster insights to clustering_insights.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"× Error saving cluster insights: {str(e)}\")\n",
    "\n",
    "# Execute clustering analysis\n",
    "print(\"1. Preparing features for clustering...\")\n",
    "clustering_features = prepare_clustering_features()\n",
    "\n",
    "print(\"\\n2. Performing clustering analysis...\")\n",
    "clustered_data, metrics, X, final_kmeans = perform_clustering_analysis(clustering_features)\n",
    "\n",
    "print(\"\\n3. Clustering Results:\")\n",
    "print(f\"✓ Optimal number of clusters: {len(clustered_data['Cluster'].unique())}\")\n",
    "print(f\"✓ Davies-Bouldin Index: {metrics.iloc[metrics['db_score'].idxmin()]['db_score']:.4f}\")\n",
    "print(f\"✓ Silhouette Score: {metrics.iloc[metrics['db_score'].idxmin()]['silhouette_score']:.4f}\")\n",
    "print(f\"✓ Calinski-Harabasz Score: {metrics.iloc[metrics['db_score'].idxmin()]['calinski_harabasz_score']:.4f}\")\n",
    "\n",
    "print(\"\\n4. Cluster sizes:\")\n",
    "print(clustered_data['Cluster'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n5. Creating visualizations and saving results...\")\n",
    "visualize_clusters(clustered_data, X, final_kmeans)\n",
    "\n",
    "# Save final results\n",
    "try:\n",
    "    clustered_data.to_csv('FirstName_LastName_Clustering_Results.csv', index=False)\n",
    "    print(\"✓ Saved clustering results to FirstName_LastName_Clustering_Results.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"× Error saving clustering results: {str(e)}\")\n",
    "\n",
    "try:\n",
    "    metrics.to_csv('FirstName_LastName_Clustering_Metrics.csv', index=False)\n",
    "    print(\"✓ Saved clustering metrics to FirstName_LastName_Clustering_Metrics.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"× Error saving clustering metrics: {str(e)}\")\n",
    "\n",
    "print(\"\\nClustering analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45449927-7b30-4b5d-889b-54a3c1a91acc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
