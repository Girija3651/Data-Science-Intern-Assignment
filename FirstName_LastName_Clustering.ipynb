{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3fbb32-ecf5-4e97-a71f-3ef437ad5ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Preparing features for clustering...\n",
      "✓ Prepared features for 200 customers\n",
      "\n",
      "2. Performing clustering analysis...\n",
      "✓ Evaluated clustering with 2 clusters\n",
      "✓ Evaluated clustering with 3 clusters\n",
      "✓ Evaluated clustering with 4 clusters\n",
      "✓ Evaluated clustering with 5 clusters\n",
      "✓ Evaluated clustering with 6 clusters\n",
      "✓ Evaluated clustering with 7 clusters\n",
      "✓ Evaluated clustering with 8 clusters\n",
      "✓ Evaluated clustering with 9 clusters\n",
      "✓ Evaluated clustering with 10 clusters\n",
      "\n",
      "3. Clustering Results:\n",
      "✓ Optimal number of clusters: 2\n",
      "✓ Davies-Bouldin Index: 0.4487\n",
      "✓ Silhouette Score: 0.6746\n",
      "✓ Calinski-Harabasz Score: 759.2050\n",
      "\n",
      "4. Cluster sizes:\n",
      "Cluster\n",
      "0     97\n",
      "1    103\n",
      "Name: count, dtype: int64\n",
      "\n",
      "5. Creating visualizations and saving results...\n",
      "\n",
      "Generating visualizations...\n",
      "✓ Created PCA visualization (clustering_pca.png)\n",
      "✓ Created cluster sizes visualization (clustering_sizes.png)\n",
      "× Error creating characteristics heatmap: \"Columns not found: 'monetary'\"\n",
      "× Error saving cluster insights: 'Column not found: monetary'\n",
      "✓ Saved clustering results to FirstName_LastName_Clustering_Results.csv\n",
      "✓ Saved clustering metrics to FirstName_LastName_Clustering_Metrics.csv\n",
      "\n",
      "Clustering analysis completed!\n"
     ]
    }
   ],
   "source": [
    "def prepare_clustering_features(customers_df=None, transactions_df=None, products_df=None):\n",
    "    \"\"\"\n",
    "    Prepare features for customer segmentation clustering using the three provided datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    customers_df (pandas.DataFrame): Customer information\n",
    "    transactions_df (pandas.DataFrame): Transaction data\n",
    "    products_df (pandas.DataFrame): Product information\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Prepared features for clustering\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if any(df is None for df in [customers_df, transactions_df, products_df]):\n",
    "            # Load your data files\n",
    "            customers_df = pd.read_csv('Customers.csv')\n",
    "            transactions_df = pd.read_csv('Transactions.csv')\n",
    "            products_df = pd.read_csv('Products.csv')\n",
    "        \n",
    "        # Initialize features DataFrame\n",
    "        features = pd.DataFrame()\n",
    "        \n",
    "        # 1. Basic customer features\n",
    "        customers_df['SignupDate'] = pd.to_datetime(customers_df['SignupDate'])\n",
    "        features['account_age_days'] = (pd.Timestamp.now() - customers_df['SignupDate']).dt.days\n",
    "        \n",
    "        # 2. Transaction-based features (RFM)\n",
    "        # Recency\n",
    "        transactions_df['TransactionDate'] = pd.to_datetime(transactions_df['TransactionDate'])\n",
    "        latest_date = transactions_df['TransactionDate'].max()\n",
    "        customer_last_purchase = transactions_df.groupby('CustomerID')['TransactionDate'].max()\n",
    "        features['recency'] = (latest_date - customer_last_purchase).dt.days\n",
    "        \n",
    "        # Frequency\n",
    "        features['frequency'] = transactions_df.groupby('CustomerID')['TransactionID'].count()\n",
    "        \n",
    "        # Monetary\n",
    "        customer_totals = transactions_df.groupby('CustomerID').agg({\n",
    "            'TotalValue': ['sum', 'mean', 'std'],\n",
    "            'Quantity': ['sum', 'mean', 'std']\n",
    "        })\n",
    "        \n",
    "        # Flatten column names\n",
    "        customer_totals.columns = [\n",
    "            'monetary_total', 'avg_transaction_value', 'std_transaction_value',\n",
    "            'total_items', 'avg_items_per_transaction', 'std_items'\n",
    "        ]\n",
    "        \n",
    "        # Add to features\n",
    "        for col in customer_totals.columns:\n",
    "            features[col] = customer_totals[col]\n",
    "        \n",
    "        # 3. Product category preferences\n",
    "        # Merge transactions with products to get categories\n",
    "        trans_with_categories = transactions_df.merge(\n",
    "            products_df[['ProductID', 'Category']], \n",
    "            on='ProductID'\n",
    "        )\n",
    "        \n",
    "        # Calculate category preferences\n",
    "        category_pivot = pd.pivot_table(\n",
    "            trans_with_categories,\n",
    "            index='CustomerID',\n",
    "            columns='Category',\n",
    "            values='Quantity',\n",
    "            aggfunc='sum',\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        # Normalize category preferences\n",
    "        category_sums = category_pivot.sum(axis=1)\n",
    "        category_preferences = category_pivot.div(category_sums, axis=0).fillna(0)\n",
    "        \n",
    "        # Add category preferences to features\n",
    "        for col in category_preferences.columns:\n",
    "            features[f'category_pref_{col}'] = category_preferences[col]\n",
    "        \n",
    "        # Handle missing values\n",
    "        features = features.fillna(0)\n",
    "        \n",
    "        # Scale the features\n",
    "        scaler = StandardScaler()\n",
    "        features_scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(features),\n",
    "            columns=features.columns,\n",
    "            index=features.index\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Prepared features for {len(features)} customers\")\n",
    "        return features_scaled\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"× Error preparing clustering features: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def perform_clustering_analysis(features, max_clusters=10):\n",
    "    \"\"\"\n",
    "    Perform K-means clustering analysis with different numbers of clusters\n",
    "    and evaluate using multiple metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    features (pandas.DataFrame): Prepared and scaled features for clustering\n",
    "    max_clusters (int): Maximum number of clusters to try\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (clustered_data, metrics_df, X, best_kmeans)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert features to numpy array for clustering\n",
    "        X = features.values\n",
    "        \n",
    "        # Initialize metrics storage\n",
    "        metrics_list = []\n",
    "        \n",
    "        # Try different numbers of clusters\n",
    "        for n_clusters in range(2, max_clusters + 1):\n",
    "            # Perform K-means clustering\n",
    "            kmeans = KMeans(\n",
    "                n_clusters=n_clusters,\n",
    "                n_init=10,\n",
    "                random_state=42\n",
    "            )\n",
    "            cluster_labels = kmeans.fit_predict(X)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics_list.append({\n",
    "                'n_clusters': n_clusters,\n",
    "                'db_score': davies_bouldin_score(X, cluster_labels),\n",
    "                'silhouette_score': silhouette_score(X, cluster_labels),\n",
    "                'calinski_harabasz_score': calinski_harabasz_score(X, cluster_labels)\n",
    "            })\n",
    "            \n",
    "            print(f\"✓ Evaluated clustering with {n_clusters} clusters\")\n",
    "        \n",
    "        # Convert metrics to DataFrame\n",
    "        metrics_df = pd.DataFrame(metrics_list)\n",
    "        \n",
    "        # Find optimal number of clusters (using Davies-Bouldin Index)\n",
    "        optimal_clusters = metrics_df.loc[metrics_df['db_score'].idxmin()]['n_clusters']\n",
    "        \n",
    "        # Perform final clustering with optimal number of clusters\n",
    "        final_kmeans = KMeans(\n",
    "            n_clusters=int(optimal_clusters),\n",
    "            n_init=10,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Add cluster assignments to original features\n",
    "        clustered_data = features.copy()\n",
    "        clustered_data['Cluster'] = final_kmeans.fit_predict(X)\n",
    "        \n",
    "        return clustered_data, metrics_df, X, final_kmeans\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"× Error performing clustering analysis: {str(e)}\\\")\")\n",
    "        raise\n",
    "\n",
    "# Modified execution code\n",
    "# Load data\n",
    "customers_df = pd.read_csv('Customers.csv')\n",
    "transactions_df = pd.read_csv('Transactions.csv')\n",
    "products_df = pd.read_csv('Products.csv')\n",
    "\n",
    "# Execute clustering analysis\n",
    "print(\"1. Preparing features for clustering...\")\n",
    "clustering_features = prepare_clustering_features(customers_df, transactions_df, products_df)\n",
    "\n",
    "print(\"\\n2. Performing clustering analysis...\")\n",
    "clustered_data, metrics, X, final_kmeans = perform_clustering_analysis(clustering_features)\n",
    "\n",
    "print(\"\\n3. Clustering Results:\")\n",
    "print(f\"✓ Optimal number of clusters: {len(clustered_data['Cluster'].unique())}\")\n",
    "print(f\"✓ Davies-Bouldin Index: {metrics.iloc[metrics['db_score'].idxmin()]['db_score']:.4f}\")\n",
    "print(f\"✓ Silhouette Score: {metrics.iloc[metrics['db_score'].idxmin()]['silhouette_score']:.4f}\")\n",
    "print(f\"✓ Calinski-Harabasz Score: {metrics.iloc[metrics['db_score'].idxmin()]['calinski_harabasz_score']:.4f}\")\n",
    "\n",
    "# Rest of the visualization and saving code remains the same...\n",
    "\n",
    "print(\"\\n4. Cluster sizes:\")\n",
    "print(clustered_data['Cluster'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n5. Creating visualizations and saving results...\")\n",
    "visualize_clusters(clustered_data, X, final_kmeans)\n",
    "\n",
    "# Save final results\n",
    "try:\n",
    "    clustered_data.to_csv('FirstName_LastName_Clustering_Results.csv', index=False)\n",
    "    print(\"✓ Saved clustering results to FirstName_LastName_Clustering_Results.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"× Error saving clustering results: {str(e)}\")\n",
    "\n",
    "try:\n",
    "    metrics.to_csv('FirstName_LastName_Clustering_Metrics.csv', index=False)\n",
    "    print(\"✓ Saved clustering metrics to FirstName_LastName_Clustering_Metrics.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"× Error saving clustering metrics: {str(e)}\")\n",
    "\n",
    "print(\"\\nClustering analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c7e440-9329-45f1-b83d-fd5948411c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
